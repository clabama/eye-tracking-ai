{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1a71f7ce",
   "metadata": {},
   "source": [
    "# Image clustering and label mapping (per_image_descriptive_summary)\n",
    "\n",
    "This notebook clusters images using features from `per_image_descriptive_summary.csv` and maps/apply labels via `labels_per_id.csv`.\n",
    "\n",
    "It will:\n",
    "- Load and prepare per-image features.\n",
    "- Standardize and cluster images (K-Means by default).\n",
    "- Visualize clusters in 2D (PCA).\n",
    "- Join labels by `image_id` from `labels_per_id.csv`.\n",
    "- Provide a simple override workflow to apply corrected labels and save them for later comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f2493d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Project root: c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\n",
      "Summary CSV: c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\descriptive_analysis\\per_image_descriptive_summary_pretty.csv (exists=True)\n",
      "Labels CSV: c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\labels_per_id.csv (exists=True)\n",
      "Outputs: c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\n"
     ]
    }
   ],
   "source": [
    "# Setup and paths\n",
    "import os, re, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, normalize as sk_normalize\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import silhouette_score\n",
    "from collections import defaultdict, Counter\n",
    "import networkx as nx\n",
    "from matplotlib.patches import FancyBboxPatch\n",
    "\n",
    "sns.set_context('talk')\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "def find_project_root(start: Path):\n",
    "    for cand in [start, *start.parents]:\n",
    "        if (cand / 'labels_per_id.csv').exists() or (cand / 'data_analysis').exists():\n",
    "            return cand\n",
    "    return start\n",
    "\n",
    "nb_dir = Path.cwd()\n",
    "project_root = find_project_root(nb_dir)\n",
    "summary_csv = project_root / 'data_analysis' / 'descriptive_analysis' / 'per_image_descriptive_summary_pretty.csv'\n",
    "labels_csv = project_root / 'labels_per_id.csv'\n",
    "out_dir = project_root / 'data_analysis' / 'label_analysis' / 'outputs'\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f'Project root: {project_root}')\n",
    "print(f'Summary CSV: {summary_csv} (exists={summary_csv.exists()})')\n",
    "print(f'Labels CSV: {labels_csv} (exists={labels_csv.exists()})')\n",
    "print(f'Outputs: {out_dir}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "7302621e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152, 21)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>primary_label</th>\n",
       "      <th>fixations_total</th>\n",
       "      <th>fixations_first_third</th>\n",
       "      <th>fixations_last_third</th>\n",
       "      <th>view_time_total_sum_ms</th>\n",
       "      <th>fixation_duration_mean_ms</th>\n",
       "      <th>fixation_duration_median_ms</th>\n",
       "      <th>fix_dur_mean_first_third_ms</th>\n",
       "      <th>fix_dur_mean_last_third_ms</th>\n",
       "      <th>...</th>\n",
       "      <th>bcea68_mean_px2</th>\n",
       "      <th>bcea95_mean_px2</th>\n",
       "      <th>pupil_mm_mean</th>\n",
       "      <th>pupil_mm_std</th>\n",
       "      <th>pupil_norm_mean</th>\n",
       "      <th>pupil_norm_std</th>\n",
       "      <th>pupil_norm_abs_mean</th>\n",
       "      <th>pupil_norm_rms</th>\n",
       "      <th>pupil_size_norm_n</th>\n",
       "      <th>avg_pupil_size_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>meme</td>\n",
       "      <td>1159</td>\n",
       "      <td>406</td>\n",
       "      <td>378</td>\n",
       "      <td>425375.0</td>\n",
       "      <td>284.3</td>\n",
       "      <td>232.4</td>\n",
       "      <td>274.6</td>\n",
       "      <td>311.5</td>\n",
       "      <td>...</td>\n",
       "      <td>72009.0</td>\n",
       "      <td>189497.0</td>\n",
       "      <td>3.722</td>\n",
       "      <td>0.172</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.795</td>\n",
       "      <td>0.970</td>\n",
       "      <td>23.632653</td>\n",
       "      <td>23.653061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>meme</td>\n",
       "      <td>1284</td>\n",
       "      <td>440</td>\n",
       "      <td>434</td>\n",
       "      <td>452560.0</td>\n",
       "      <td>274.6</td>\n",
       "      <td>232.9</td>\n",
       "      <td>270.6</td>\n",
       "      <td>286.0</td>\n",
       "      <td>...</td>\n",
       "      <td>60359.0</td>\n",
       "      <td>158841.0</td>\n",
       "      <td>3.706</td>\n",
       "      <td>0.146</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.782</td>\n",
       "      <td>0.976</td>\n",
       "      <td>26.204082</td>\n",
       "      <td>26.204082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>meme</td>\n",
       "      <td>1110</td>\n",
       "      <td>415</td>\n",
       "      <td>350</td>\n",
       "      <td>411436.0</td>\n",
       "      <td>296.9</td>\n",
       "      <td>245.3</td>\n",
       "      <td>261.3</td>\n",
       "      <td>375.5</td>\n",
       "      <td>...</td>\n",
       "      <td>40009.0</td>\n",
       "      <td>105288.0</td>\n",
       "      <td>3.839</td>\n",
       "      <td>0.182</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.797</td>\n",
       "      <td>0.974</td>\n",
       "      <td>23.104167</td>\n",
       "      <td>23.125000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>meme</td>\n",
       "      <td>1354</td>\n",
       "      <td>477</td>\n",
       "      <td>445</td>\n",
       "      <td>469790.0</td>\n",
       "      <td>280.2</td>\n",
       "      <td>216.6</td>\n",
       "      <td>269.5</td>\n",
       "      <td>306.1</td>\n",
       "      <td>...</td>\n",
       "      <td>87693.0</td>\n",
       "      <td>230772.0</td>\n",
       "      <td>3.761</td>\n",
       "      <td>0.164</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.792</td>\n",
       "      <td>0.977</td>\n",
       "      <td>27.632653</td>\n",
       "      <td>27.632653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>meme</td>\n",
       "      <td>1359</td>\n",
       "      <td>480</td>\n",
       "      <td>435</td>\n",
       "      <td>460515.0</td>\n",
       "      <td>265.5</td>\n",
       "      <td>216.4</td>\n",
       "      <td>255.6</td>\n",
       "      <td>281.6</td>\n",
       "      <td>...</td>\n",
       "      <td>90696.0</td>\n",
       "      <td>238674.0</td>\n",
       "      <td>4.008</td>\n",
       "      <td>0.204</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.776</td>\n",
       "      <td>0.975</td>\n",
       "      <td>27.734694</td>\n",
       "      <td>27.734694</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id primary_label  fixations_total  fixations_first_third  \\\n",
       "0      001          meme             1159                    406   \n",
       "1      002          meme             1284                    440   \n",
       "2      003          meme             1110                    415   \n",
       "3      004          meme             1354                    477   \n",
       "4      005          meme             1359                    480   \n",
       "\n",
       "   fixations_last_third  view_time_total_sum_ms  fixation_duration_mean_ms  \\\n",
       "0                   378                425375.0                      284.3   \n",
       "1                   434                452560.0                      274.6   \n",
       "2                   350                411436.0                      296.9   \n",
       "3                   445                469790.0                      280.2   \n",
       "4                   435                460515.0                      265.5   \n",
       "\n",
       "   fixation_duration_median_ms  fix_dur_mean_first_third_ms  \\\n",
       "0                        232.4                        274.6   \n",
       "1                        232.9                        270.6   \n",
       "2                        245.3                        261.3   \n",
       "3                        216.6                        269.5   \n",
       "4                        216.4                        255.6   \n",
       "\n",
       "   fix_dur_mean_last_third_ms  ...  bcea68_mean_px2  bcea95_mean_px2  \\\n",
       "0                       311.5  ...          72009.0         189497.0   \n",
       "1                       286.0  ...          60359.0         158841.0   \n",
       "2                       375.5  ...          40009.0         105288.0   \n",
       "3                       306.1  ...          87693.0         230772.0   \n",
       "4                       281.6  ...          90696.0         238674.0   \n",
       "\n",
       "   pupil_mm_mean  pupil_mm_std  pupil_norm_mean  pupil_norm_std  \\\n",
       "0          3.722         0.172              0.0             1.0   \n",
       "1          3.706         0.146             -0.0             1.0   \n",
       "2          3.839         0.182              0.0             1.0   \n",
       "3          3.761         0.164             -0.0             1.0   \n",
       "4          4.008         0.204             -0.0             1.0   \n",
       "\n",
       "   pupil_norm_abs_mean  pupil_norm_rms  pupil_size_norm_n  avg_pupil_size_n  \n",
       "0                0.795           0.970          23.632653         23.653061  \n",
       "1                0.782           0.976          26.204082         26.204082  \n",
       "2                0.797           0.974          23.104167         23.125000  \n",
       "3                0.792           0.977          27.632653         27.632653  \n",
       "4                0.776           0.975          27.734694         27.734694  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data\n",
    "if not summary_csv.exists():\n",
    "    raise FileNotFoundError(f'Missing {summary_csv}')\n",
    "df = pd.read_csv(summary_csv)\n",
    "# Ensure image_id as 3-digit string\n",
    "if 'image_id' in df.columns:\n",
    "    df['image_id'] = df['image_id'].astype(str).str.extract(r'(\\d+)').fillna('').iloc[:,0].str.zfill(3)\n",
    "else:\n",
    "    raise KeyError('image_id column not found in per_image_descriptive_summary_pretty.csv')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd967833",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used (19): ['fixations_total', 'fixations_first_third', 'fixations_last_third', 'view_time_total_sum_ms', 'fixation_duration_mean_ms', 'fixation_duration_median_ms', 'fix_dur_mean_first_third_ms', 'fix_dur_mean_last_third_ms', 'scanpath_length_mean_px', 'bcea68_mean_px2'] ...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>image_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-2.127166</td>\n",
       "      <td>0.321788</td>\n",
       "      <td>001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1.160300</td>\n",
       "      <td>-1.123852</td>\n",
       "      <td>002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-3.332609</td>\n",
       "      <td>-0.905180</td>\n",
       "      <td>003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.295759</td>\n",
       "      <td>0.507351</td>\n",
       "      <td>004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.325878</td>\n",
       "      <td>1.101427</td>\n",
       "      <td>005</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        PC1       PC2 image_id\n",
       "0 -2.127166  0.321788      001\n",
       "1 -1.160300 -1.123852      002\n",
       "2 -3.332609 -0.905180      003\n",
       "3 -0.295759  0.507351      004\n",
       "4 -0.325878  1.101427      005"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature selection and preprocessing\n",
    "# Pick numeric columns automatically, exclude obvious identifiers\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "exclude = {'image_id', }\n",
    "feature_cols = [c for c in num_cols if c not in exclude]\n",
    "if not feature_cols:\n",
    "    raise ValueError('No numeric feature columns found for clustering.')\n",
    "X = df[feature_cols].copy()\n",
    "# Fill missing with column medians\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "# 2D projection for visualization\n",
    "pca = PCA(n_components=2, random_state=42)\n",
    "Z = pca.fit_transform(X_scaled)\n",
    "print(f'Features used ({len(feature_cols)}):', feature_cols[:10], '...')\n",
    "pd.DataFrame({'PC1': Z[:,0], 'PC2': Z[:,1], 'image_id': df['image_id']}).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b4eb609a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finding optimal k...\n",
      "Saved k analysis plot -> c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\k_selection_analysis.png\n",
      "\n",
      "K selection results:\n",
      "  Elbow method chose: k=4 (provides reasonable cluster quantity)\n",
      "  Silhouette score for k=4: 0.190 (evaluates clustering quality)\n",
      "  Best possible silhouette was: 0.431 at k=2\n",
      "  Quality assessment: Poor clustering quality - consider different approach\n",
      "Saved centroids -> c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\cluster_feature_centroids.csv\n",
      "Saved k analysis -> c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\k_selection_results.json\n",
      "Saved k analysis plot -> c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\k_selection_analysis.png\n",
      "\n",
      "K selection results:\n",
      "  Elbow method chose: k=4 (provides reasonable cluster quantity)\n",
      "  Silhouette score for k=4: 0.190 (evaluates clustering quality)\n",
      "  Best possible silhouette was: 0.431 at k=2\n",
      "  Quality assessment: Poor clustering quality - consider different approach\n",
      "Saved centroids -> c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\cluster_feature_centroids.csv\n",
      "Saved k analysis -> c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\k_selection_results.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>cluster</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.127166</td>\n",
       "      <td>0.321788</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.160300</td>\n",
       "      <td>-1.123852</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.332609</td>\n",
       "      <td>-0.905180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.295759</td>\n",
       "      <td>0.507351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.325878</td>\n",
       "      <td>1.101427</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  cluster       PC1       PC2\n",
       "0      001        2 -2.127166  0.321788\n",
       "1      002        2 -1.160300 -1.123852\n",
       "2      003        0 -3.332609 -0.905180\n",
       "3      004        2 -0.295759  0.507351\n",
       "4      005        2 -0.325878  1.101427"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimal k selection using elbow method and silhouette score\n",
    "def find_optimal_k(X, k_range=range(2, 11), random_state=42):\n",
    "    \"\"\"Find optimal k using elbow method and silhouette score.\"\"\"\n",
    "    inertias = []\n",
    "    silhouettes = []\n",
    "    k_values = list(k_range)\n",
    "    \n",
    "    for k in k_values:\n",
    "        km = KMeans(n_clusters=k, n_init=25, random_state=random_state)\n",
    "        clusters = km.fit_predict(X)\n",
    "        inertias.append(km.inertia_)\n",
    "        sil_score = silhouette_score(X, clusters)\n",
    "        silhouettes.append(sil_score)\n",
    "    \n",
    "    # Elbow method: find point with maximum curvature\n",
    "    # Using second derivative approximation\n",
    "    if len(k_values) >= 3:\n",
    "        second_derivs = []\n",
    "        for i in range(1, len(inertias) - 1):\n",
    "            second_deriv = inertias[i-1] - 2*inertias[i] + inertias[i+1]\n",
    "            second_derivs.append(second_deriv)\n",
    "        elbow_idx = np.argmax(second_derivs) + 1  # +1 because we start from index 1\n",
    "        elbow_k = k_values[elbow_idx]\n",
    "    else:\n",
    "        elbow_k = k_values[0]\n",
    "    \n",
    "    # Best silhouette score\n",
    "    sil_idx = np.argmax(silhouettes)\n",
    "    sil_k = k_values[sil_idx]\n",
    "    \n",
    "    return {\n",
    "        'k_values': k_values,\n",
    "        'inertias': inertias,\n",
    "        'silhouettes': silhouettes,\n",
    "        'elbow_k': elbow_k,\n",
    "        'silhouette_k': sil_k,\n",
    "        'elbow_score': silhouettes[elbow_idx] if len(k_values) >= 3 else silhouettes[0],\n",
    "        'max_silhouette': max(silhouettes)\n",
    "    }\n",
    "\n",
    "# Find optimal k\n",
    "print(\"Finding optimal k...\")\n",
    "k_analysis = find_optimal_k(X_scaled, k_range=range(2, min(12, len(df)//2 + 1)))\n",
    "\n",
    "# Plot elbow and silhouette curves\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Elbow plot\n",
    "ax1.plot(k_analysis['k_values'], k_analysis['inertias'], 'bo-')\n",
    "ax1.axvline(k_analysis['elbow_k'], color='red', linestyle='--', alpha=0.7, \n",
    "           label=f'Chosen k={k_analysis[\"elbow_k\"]}')\n",
    "ax1.set_xlabel('k (number of clusters)')\n",
    "ax1.set_ylabel('Inertia (within-cluster sum of squares)')\n",
    "ax1.set_title('Elbow Method (Quantity Selection)')\n",
    "ax1.legend()\n",
    "ax1.grid(True)\n",
    "\n",
    "# Silhouette plot\n",
    "ax2.plot(k_analysis['k_values'], k_analysis['silhouettes'], 'go-')\n",
    "ax2.axvline(k_analysis['elbow_k'], color='red', linestyle='--', alpha=0.7,\n",
    "           label=f'Elbow-chosen k={k_analysis[\"elbow_k\"]}')\n",
    "ax2.axvline(k_analysis['silhouette_k'], color='orange', linestyle=':', alpha=0.7,\n",
    "           label=f'Best silhouette k={k_analysis[\"silhouette_k\"]}')\n",
    "ax2.set_xlabel('k (number of clusters)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Analysis (Quality Evaluation)')\n",
    "ax2.legend()\n",
    "ax2.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "k_analysis_png = out_dir / 'k_selection_analysis.png'\n",
    "plt.savefig(k_analysis_png, dpi=150)\n",
    "plt.close()\n",
    "print(f'Saved k analysis plot -> {k_analysis_png}')\n",
    "\n",
    "# Choose k using Elbow Method (for quantity) and evaluate quality with Silhouette Score\n",
    "# The Elbow Method determines the reasonable number of clusters\n",
    "chosen_k = k_analysis['elbow_k']\n",
    "elbow_silhouette = k_analysis['elbow_score']\n",
    "\n",
    "# Fit final KMeans with elbow-chosen k\n",
    "km = KMeans(n_clusters=chosen_k, n_init=25, random_state=42)\n",
    "clusters = km.fit_predict(X_scaled)\n",
    "final_sil = silhouette_score(X_scaled, clusters)\n",
    "\n",
    "print(f\"\\nK selection results:\")\n",
    "print(f\"  Elbow method chose: k={chosen_k} (provides reasonable cluster quantity)\")\n",
    "print(f\"  Silhouette score for k={chosen_k}: {final_sil:.3f} (evaluates clustering quality)\")\n",
    "print(f\"  Best possible silhouette was: {k_analysis['max_silhouette']:.3f} at k={k_analysis['silhouette_k']}\")\n",
    "\n",
    "# Quality assessment\n",
    "if final_sil > 0.7:\n",
    "    quality_assessment = \"Excellent clustering quality\"\n",
    "elif final_sil > 0.5:\n",
    "    quality_assessment = \"Good clustering quality\"\n",
    "elif final_sil > 0.25:\n",
    "    quality_assessment = \"Fair clustering quality\"\n",
    "else:\n",
    "    quality_assessment = \"Poor clustering quality - consider different approach\"\n",
    "\n",
    "print(f\"  Quality assessment: {quality_assessment}\")\n",
    "\n",
    "# Build per-image cluster assignment (include PCA coords for convenience)\n",
    "cluster_df = pd.DataFrame({\n",
    "    'image_id': df['image_id'],\n",
    "    'cluster': clusters,\n",
    "    'PC1': Z[:, 0],\n",
    "    'PC2': Z[:, 1],\n",
    "})\n",
    "\n",
    "# Save centroids in original feature space for reference\n",
    "centroids_scaled = km.cluster_centers_\n",
    "centroids = pd.DataFrame(scaler.inverse_transform(centroids_scaled), columns=feature_cols)\n",
    "centroids.insert(0, 'cluster', np.arange(chosen_k))\n",
    "centroids_out = out_dir / 'cluster_feature_centroids.csv'\n",
    "centroids.to_csv(centroids_out, index=False)\n",
    "print(f'Saved centroids -> {centroids_out}')\n",
    "\n",
    "# Save k analysis results\n",
    "k_analysis_out = out_dir / 'k_selection_results.json'\n",
    "with open(k_analysis_out, 'w') as f:\n",
    "    json.dump({k: v for k, v in k_analysis.items() if k != 'k_values'}, f, indent=2)\n",
    "print(f'Saved k analysis -> {k_analysis_out}')\n",
    "\n",
    "cluster_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "0024ab64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\image_clusters_pca.png\n"
     ]
    }
   ],
   "source": [
    "# PCA scatter colored by cluster\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.scatterplot(x=Z[:,0], y=Z[:,1], hue=clusters, palette='tab10', s=60, edgecolor='white', linewidth=0.5)\n",
    "for i, img in enumerate(df['image_id']):\n",
    "    plt.text(Z[i,0], Z[i,1], img, fontsize=7, alpha=0.8, ha='center', va='center')\n",
    "plt.title(f'Images clustered (k={chosen_k}, silhouette={final_sil:.3f})\\nPCA 2D projection')\n",
    "plt.xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "plt.ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "plt.legend(title='cluster', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "pca_png = out_dir / 'image_clusters_pca.png'\n",
    "plt.savefig(pca_png, dpi=150, bbox_inches='tight')\n",
    "plt.close()\n",
    "print(f'Saved {pca_png}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "49b956b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label normalization examples:\n",
      "  'ort (text)' -> 'ort text'\n",
      "  'person (text)' -> 'person text'\n",
      "  'person (text)' -> 'person text'\n",
      "  'person (text)' -> 'person text'\n",
      "  'person (text)' -> 'person text'\n",
      "Labels loaded: (154, 15)\n",
      "Saved c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\image_clusters_with_labels.csv (rows=152)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>cluster</th>\n",
       "      <th>PC1</th>\n",
       "      <th>PC2</th>\n",
       "      <th>labels_txt</th>\n",
       "      <th>strong_tags</th>\n",
       "      <th>weak_tags</th>\n",
       "      <th>meme</th>\n",
       "      <th>person</th>\n",
       "      <th>politik</th>\n",
       "      <th>ort</th>\n",
       "      <th>text</th>\n",
       "      <th>meme_weight</th>\n",
       "      <th>person_weight</th>\n",
       "      <th>politik_weight</th>\n",
       "      <th>ort_weight</th>\n",
       "      <th>text_weight</th>\n",
       "      <th>labels_txt_normalized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>2</td>\n",
       "      <td>-2.127166</td>\n",
       "      <td>0.321788</td>\n",
       "      <td>meme</td>\n",
       "      <td>meme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>2</td>\n",
       "      <td>-1.160300</td>\n",
       "      <td>-1.123852</td>\n",
       "      <td>meme</td>\n",
       "      <td>meme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>0</td>\n",
       "      <td>-3.332609</td>\n",
       "      <td>-0.905180</td>\n",
       "      <td>meme</td>\n",
       "      <td>meme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.295759</td>\n",
       "      <td>0.507351</td>\n",
       "      <td>meme</td>\n",
       "      <td>meme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>2</td>\n",
       "      <td>-0.325878</td>\n",
       "      <td>1.101427</td>\n",
       "      <td>meme</td>\n",
       "      <td>meme</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  cluster       PC1       PC2 labels_txt strong_tags weak_tags  \\\n",
       "0      001        2 -2.127166  0.321788       meme        meme       NaN   \n",
       "1      002        2 -1.160300 -1.123852       meme        meme       NaN   \n",
       "2      003        0 -3.332609 -0.905180       meme        meme       NaN   \n",
       "3      004        2 -0.295759  0.507351       meme        meme       NaN   \n",
       "4      005        2 -0.325878  1.101427       meme        meme       NaN   \n",
       "\n",
       "   meme  person  politik  ort  text  meme_weight  person_weight  \\\n",
       "0     1       0        0    0     0          1.0            0.0   \n",
       "1     1       0        0    0     0          1.0            0.0   \n",
       "2     1       0        0    0     0          1.0            0.0   \n",
       "3     1       0        0    0     0          1.0            0.0   \n",
       "4     1       0        0    0     0          1.0            0.0   \n",
       "\n",
       "   politik_weight  ort_weight  text_weight labels_txt_normalized  \n",
       "0             0.0         0.0          0.0                  meme  \n",
       "1             0.0         0.0          0.0                  meme  \n",
       "2             0.0         0.0          0.0                  meme  \n",
       "3             0.0         0.0          0.0                  meme  \n",
       "4             0.0         0.0          0.0                  meme  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Join labels_per_id.csv by image_id\n",
    "labels_df = None\n",
    "if labels_csv.exists():\n",
    "    labels_df = pd.read_csv(labels_csv)\n",
    "    if 'image_id' in labels_df.columns:\n",
    "        labels_df['image_id'] = labels_df['image_id'].astype(str).str.extract(r'(\\d+)').fillna('').iloc[:,0].str.zfill(3)\n",
    "    else:\n",
    "        for c in ['id','image','img_id']:\n",
    "            if c in labels_df.columns:\n",
    "                labels_df['image_id'] = labels_df[c].astype(str).str.extract(r'(\\d+)').fillna('').iloc[:,0].str.zfill(3)\n",
    "                break\n",
    "    \n",
    "    # Normalize labels: remove parentheses from low-weight tags for better overview\n",
    "    # Convert \"meme text (politik)\" -> \"meme text politik\", \"person (text)\" -> \"person text\", etc.\n",
    "    if 'labels_txt' in labels_df.columns:\n",
    "        labels_df['labels_txt_normalized'] = labels_df['labels_txt'].str.replace(r'\\s*\\([^)]+\\)', '', regex=True)\n",
    "        labels_df['labels_txt_normalized'] = labels_df['labels_txt_normalized'].str.replace(r'\\([^)]+\\)\\s*', '', regex=True)\n",
    "        # Clean up extra spaces and rebuild with consistent format\n",
    "        def normalize_label(label):\n",
    "            if pd.isna(label):\n",
    "                return label\n",
    "            # Extract individual components\n",
    "            parts = []\n",
    "            # Remove parentheses and split by common separators\n",
    "            clean_label = str(label).replace('(', ' ').replace(')', ' ')\n",
    "            for part in clean_label.replace(',', ' ').split():\n",
    "                part = part.strip()\n",
    "                if part and part not in parts:\n",
    "                    parts.append(part)\n",
    "            return ' '.join(sorted(parts)) if parts else label\n",
    "        \n",
    "        labels_df['labels_txt_normalized'] = labels_df['labels_txt'].apply(normalize_label)\n",
    "        print(f\"Label normalization examples:\")\n",
    "        examples = labels_df[labels_df['labels_txt'] != labels_df['labels_txt_normalized']][['labels_txt', 'labels_txt_normalized']].head(5)\n",
    "        if not examples.empty:\n",
    "            for _, row in examples.iterrows():\n",
    "                print(f\"  '{row['labels_txt']}' -> '{row['labels_txt_normalized']}'\")\n",
    "        else:\n",
    "            print(\"  No changes needed (no parentheses found)\")\n",
    "    \n",
    "    print(f'Labels loaded: {labels_df.shape if labels_df is not None else None}')\n",
    "else:\n",
    "    print('labels_per_id.csv not found; continuing without label join')\n",
    "\n",
    "merged = cluster_df.copy()\n",
    "if labels_df is not None:\n",
    "    merged = merged.merge(labels_df, on='image_id', how='left')\n",
    "merged_out = out_dir / 'image_clusters_with_labels.csv'\n",
    "merged.to_csv(merged_out, index=False)\n",
    "print(f'Saved {merged_out} (rows={len(merged)})')\n",
    "merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "e7e61ba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\cluster_label_crosstab.csv (using labels_txt_normalized)\n"
     ]
    }
   ],
   "source": [
    "# Cluster vs label crosstab (if a label column is present)\n",
    "label_col_guess = None\n",
    "if 'labels_txt_normalized' in merged.columns:\n",
    "    label_col_guess = 'labels_txt_normalized'\n",
    "elif 'labels_txt' in merged.columns:\n",
    "    label_col_guess = 'labels_txt'\n",
    "elif 'primary_label' in merged.columns:\n",
    "    label_col_guess = 'primary_label'\n",
    "elif 'category' in merged.columns:\n",
    "    label_col_guess = 'category'\n",
    "if label_col_guess:\n",
    "    ct = pd.crosstab(merged['cluster'], merged[label_col_guess])\n",
    "    ct_out = out_dir / 'cluster_label_crosstab.csv'\n",
    "    ct.to_csv(ct_out)\n",
    "    print(f'Saved {ct_out} (using {label_col_guess})')\n",
    "    ct\n",
    "else:\n",
    "    print('No obvious label column found to crosstab.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10e534a",
   "metadata": {},
   "source": [
    "### Apply/override labels\n",
    "Create a CSV template you can edit (column `applied_label`). Reload it to apply overrides and save a final mapping for downstream use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "b28ac23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Override template already exists: c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\label_overrides_template.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>current_label</th>\n",
       "      <th>applied_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>meme</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>meme</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>meme</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>meme</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>meme</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id current_label applied_label\n",
       "0      001          meme          meme\n",
       "1      002          meme          meme\n",
       "2      003          meme          meme\n",
       "3      004          meme          meme\n",
       "4      005          meme          meme"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create overrides template (one row per image) if not exists\n",
    "template_path = out_dir / 'label_overrides_template.csv'\n",
    "template = merged[['image_id']].drop_duplicates().copy()\n",
    "# carry over existing label guess if available (prefer normalized labels)\n",
    "if 'labels_txt_normalized' in merged.columns:\n",
    "    template['current_label'] = merged.groupby('image_id')['labels_txt_normalized'].first().reindex(template['image_id']).values\n",
    "elif 'labels_txt' in merged.columns:\n",
    "    template['current_label'] = merged.groupby('image_id')['labels_txt'].first().reindex(template['image_id']).values\n",
    "elif 'primary_label' in merged.columns:\n",
    "    template['current_label'] = merged.groupby('image_id')['primary_label'].first().reindex(template['image_id']).values\n",
    "else:\n",
    "    template['current_label'] = ''\n",
    "template['applied_label'] = template['current_label']\n",
    "if not template_path.exists():\n",
    "    template.to_csv(template_path, index=False)\n",
    "    print(f'Wrote override template -> {template_path}')\n",
    "else:\n",
    "    print(f'Override template already exists: {template_path}')\n",
    "template.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "eeec31e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final labels -> c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\image_labels_applied.csv (rows=152)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>cluster</th>\n",
       "      <th>applied_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>001</td>\n",
       "      <td>2</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>002</td>\n",
       "      <td>2</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>003</td>\n",
       "      <td>0</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>004</td>\n",
       "      <td>2</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>005</td>\n",
       "      <td>2</td>\n",
       "      <td>meme</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  image_id  cluster applied_label\n",
       "0      001        2          meme\n",
       "1      002        2          meme\n",
       "2      003        0          meme\n",
       "3      004        2          meme\n",
       "4      005        2          meme"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load overrides (edit the CSV externally, then run this cell)\n",
    "overrides = pd.read_csv(out_dir / 'label_overrides_template.csv') if (out_dir / 'label_overrides_template.csv').exists() else template.copy()\n",
    "overrides['image_id'] = overrides['image_id'].astype(str).str.extract(r'(\\d+)').fillna('').iloc[:,0].str.zfill(3)\n",
    "final_map = merged[['image_id','cluster']].drop_duplicates().merge(overrides[['image_id','applied_label']], on='image_id', how='left')\n",
    "final_out = out_dir / 'image_labels_applied.csv'\n",
    "final_map.to_csv(final_out, index=False)\n",
    "print(f'Saved final labels -> {final_out} (rows={len(final_map)})')\n",
    "final_map.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c448e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "16fcde80",
   "metadata": {},
   "source": [
    "## Cluster compositions and feature profiles\n",
    "\n",
    "The next cells visualize cluster sizes and label distributions, then profile each cluster via z-scored feature centroids and auto-generated explanations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "cf6f322a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\cluster_sizes.png\n",
      "Saved c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\cluster_label_distribution.png\n",
      "Saved c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\cluster_label_distribution.png\n"
     ]
    }
   ],
   "source": [
    "# Cluster sizes and optional label distribution\n",
    "assert 'cluster_df' in globals(), 'cluster_df missing; run clustering cell first.'\n",
    "\n",
    "# If merged (with labels) exists, use it; else fall back to cluster_df\n",
    "df_for_labels = globals().get('merged', None)\n",
    "if df_for_labels is None:\n",
    "    df_for_labels = cluster_df.copy()\n",
    "\n",
    "# Cluster size counts\n",
    "counts = cluster_df['cluster'].value_counts().sort_index()\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(x=counts.index.astype(str), y=counts.values, color='steelblue')\n",
    "plt.title('Cluster sizes (image count)')\n",
    "plt.xlabel('cluster')\n",
    "plt.ylabel('images')\n",
    "plt.tight_layout()\n",
    "bar_out = out_dir / 'cluster_sizes.png'\n",
    "plt.savefig(bar_out, dpi=150)\n",
    "plt.close()\n",
    "print(f'Saved {bar_out}')\n",
    "\n",
    "# Label distribution per cluster (if a label column exists)\n",
    "label_col_guess = None\n",
    "for cand in ['applied_label', 'labels_txt_normalized', 'labels_txt', 'primary_label', 'category']:\n",
    "    if cand in df_for_labels.columns:\n",
    "        label_col_guess = cand\n",
    "        break\n",
    "\n",
    "if label_col_guess:\n",
    "    ct = pd.crosstab(df_for_labels['cluster'], df_for_labels[label_col_guess])\n",
    "    plt.figure(figsize=(10, max(4, 0.35*ct.shape[1])))\n",
    "    (ct.T / ct.sum(axis=1)).T.plot(kind='bar', stacked=True, ax=plt.gca(), colormap='tab20')\n",
    "    plt.title(f'Label distribution per cluster ({label_col_guess})')\n",
    "    plt.xlabel('cluster')\n",
    "    plt.ylabel('share')\n",
    "    plt.legend(title=label_col_guess, bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    stacked_out = out_dir / 'cluster_label_distribution.png'\n",
    "    plt.savefig(stacked_out, dpi=150)\n",
    "    plt.close()\n",
    "    print(f'Saved {stacked_out}')\n",
    "else:\n",
    "    print('No label column found; skipped label distribution plot.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "8abcc039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\cluster_feature_profiles_heatmap.png\n",
      "Saved c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\cluster_explanations.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cluster</th>\n",
       "      <th>top_positive_features</th>\n",
       "      <th>top_negative_features</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>pupil_mm_std (+1.38), pupil_mm_mean (+1.32), f...</td>\n",
       "      <td>bcea95_mean_px2 (-1.16), bcea68_mean_px2 (-1.1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>view_time_total_sum_ms (+1.41), fixations_last...</td>\n",
       "      <td>pupil_norm_abs_mean (-1.32), fixation_duration...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>pupil_norm_abs_mean (+0.53), fixation_duration...</td>\n",
       "      <td>pupil_size_norm_n (-0.58), avg_pupil_size_n (-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>bcea68_mean_px2 (+0.96), bcea95_mean_px2 (+0.9...</td>\n",
       "      <td>fix_dur_mean_first_third_ms (-0.71), fixation_...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cluster                              top_positive_features  \\\n",
       "0        0  pupil_mm_std (+1.38), pupil_mm_mean (+1.32), f...   \n",
       "1        1  view_time_total_sum_ms (+1.41), fixations_last...   \n",
       "2        2  pupil_norm_abs_mean (+0.53), fixation_duration...   \n",
       "3        3  bcea68_mean_px2 (+0.96), bcea95_mean_px2 (+0.9...   \n",
       "\n",
       "                               top_negative_features  \n",
       "0  bcea95_mean_px2 (-1.16), bcea68_mean_px2 (-1.1...  \n",
       "1  pupil_norm_abs_mean (-1.32), fixation_duration...  \n",
       "2  pupil_size_norm_n (-0.58), avg_pupil_size_n (-...  \n",
       "3  fix_dur_mean_first_third_ms (-0.71), fixation_...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Feature profiles and auto-explanations\n",
    "# Compute cluster centroids in scaled space and z-score by feature\n",
    "assert 'km' in globals() and 'feature_cols' in globals(), 'Run clustering first.'\n",
    "\n",
    "centroids_scaled = km.cluster_centers_\n",
    "centroids_scaled_df = pd.DataFrame(centroids_scaled, columns=feature_cols)\n",
    "# z-score across clusters (per feature) for comparability\n",
    "centroids_z = (centroids_scaled_df - centroids_scaled_df.mean(axis=0)) / (centroids_scaled_df.std(axis=0) + 1e-9)\n",
    "centroids_z['cluster'] = np.arange(centroids_scaled_df.shape[0])\n",
    "centroids_z = centroids_z.set_index('cluster').sort_index()\n",
    "\n",
    "plt.figure(figsize=(max(8, 0.5*len(feature_cols)), 1.2*len(centroids_z)))\n",
    "sns.heatmap(centroids_z, cmap='coolwarm', center=0, cbar_kws={'label':'z-score (vs other clusters)'})\n",
    "plt.title('Cluster feature profiles (z-scored centroids)')\n",
    "plt.xlabel('feature')\n",
    "plt.ylabel('cluster')\n",
    "plt.tight_layout()\n",
    "heatmap_out = out_dir / 'cluster_feature_profiles_heatmap.png'\n",
    "plt.savefig(heatmap_out, dpi=150)\n",
    "plt.close()\n",
    "print(f'Saved {heatmap_out}')\n",
    "\n",
    "# Auto-generate short per-cluster explanations based on top +/- features\n",
    "explanations = []\n",
    "for c in range(centroids_z.shape[0]):\n",
    "    row = centroids_z.loc[c]\n",
    "    top_pos = row.sort_values(ascending=False).head(3)\n",
    "    top_neg = row.sort_values(ascending=True).head(3)\n",
    "    pos_feats = \", \".join([f\"{k} (+{v:.2f})\" for k, v in top_pos.items()])\n",
    "    neg_feats = \", \".join([f\"{k} ({v:.2f})\" for k, v in top_neg.items()])\n",
    "    explanations.append({\n",
    "        'cluster': c,\n",
    "        'top_positive_features': pos_feats,\n",
    "        'top_negative_features': neg_feats,\n",
    "    })\n",
    "\n",
    "exp_df = pd.DataFrame(explanations)\n",
    "exp_out = out_dir / 'cluster_explanations.csv'\n",
    "exp_df.to_csv(exp_out, index=False)\n",
    "print(f'Saved {exp_out}')\n",
    "exp_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "0e930c36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 12 unique label combinations\n",
      "Saved label tree visualization -> c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\label_hierarchy_tree.png\n",
      "Saved label statistics -> c:\\Users\\SWixforth\\Uni\\eye-tracking-ai\\data_analysis\\label_analysis\\outputs\\label_hierarchy_stats.json\n",
      "\n",
      "Label hierarchy summary:\n",
      "Base labels: meme, ort, person, politik, text\n",
      "Top 5 combinations: ['person', 'ort', 'meme text', 'person politik', 'person politik text']\n"
     ]
    }
   ],
   "source": [
    "# Hierarchical Label Tree Visualization\n",
    "def create_label_tree_visualization():\n",
    "    \"\"\"Create a tree visualization of label hierarchy and frequencies.\"\"\"\n",
    "    \n",
    "    # Get label data\n",
    "    if 'merged' not in globals() or 'labels_txt_normalized' not in merged.columns:\n",
    "        print(\"No normalized labels found. Run label processing first.\")\n",
    "        return\n",
    "    \n",
    "    # Count label frequencies\n",
    "    label_counts = merged['labels_txt_normalized'].value_counts()\n",
    "    print(f\"Found {len(label_counts)} unique label combinations\")\n",
    "    \n",
    "    # Parse labels into hierarchical structure\n",
    "    label_hierarchy = defaultdict(list)\n",
    "    base_labels = set()\n",
    "    \n",
    "    for label, count in label_counts.items():\n",
    "        if pd.isna(label):\n",
    "            continue\n",
    "        \n",
    "        # Split into individual components\n",
    "        components = sorted([c.strip() for c in str(label).split() if c.strip()])\n",
    "        \n",
    "        if len(components) == 1:\n",
    "            # Single label - this is a base/root node\n",
    "            base_labels.add(components[0])\n",
    "            label_hierarchy['root'].append((components[0], count))\n",
    "        else:\n",
    "            # Combination - find the best parent (most specific subset)\n",
    "            for base in components:\n",
    "                base_labels.add(base)\n",
    "            \n",
    "            # For combinations, we'll organize by primary component (first alphabetically)\n",
    "            primary = components[0]\n",
    "            label_hierarchy[primary].append((' '.join(components), count))\n",
    "    \n",
    "    # Create the tree visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(16, 12))\n",
    "    \n",
    "    # Calculate positions\n",
    "    base_labels_sorted = sorted(base_labels)\n",
    "    n_base = len(base_labels_sorted)\n",
    "    \n",
    "    # Position base labels horizontally\n",
    "    base_positions = {}\n",
    "    base_y = 0.8\n",
    "    spacing = 0.8 / max(1, n_base - 1) if n_base > 1 else 0\n",
    "    \n",
    "    for i, base_label in enumerate(base_labels_sorted):\n",
    "        x_pos = 0.1 + i * spacing\n",
    "        base_positions[base_label] = (x_pos, base_y)\n",
    "    \n",
    "    # Draw base labels\n",
    "    base_label_counts = {}\n",
    "    for label, count in label_counts.items():\n",
    "        if pd.isna(label):\n",
    "            continue\n",
    "        components = [c.strip() for c in str(label).split() if c.strip()]\n",
    "        if len(components) == 1:\n",
    "            base_label_counts[components[0]] = count\n",
    "    \n",
    "    # Draw nodes and connections\n",
    "    drawn_positions = {}\n",
    "    \n",
    "    # Draw base labels (top level)\n",
    "    for base_label in base_labels_sorted:\n",
    "        x, y = base_positions[base_label]\n",
    "        count = base_label_counts.get(base_label, 0)\n",
    "        \n",
    "        # Node size proportional to frequency\n",
    "        max_count = label_counts.max()\n",
    "        node_size = 20 + (count / max_count) * 80\n",
    "        \n",
    "        # Color by base label type\n",
    "        colors = {'meme': '#ff7f0e', 'person': '#2ca02c', 'ort': '#d62728', \n",
    "                 'text': '#9467bd', 'politik': '#8c564b'}\n",
    "        color = colors.get(base_label, '#1f77b4')\n",
    "        \n",
    "        # Draw node\n",
    "        circle = plt.Circle((x, y), node_size/1000, color=color, alpha=0.7, zorder=2)\n",
    "        ax.add_patch(circle)\n",
    "        \n",
    "        # Label\n",
    "        ax.text(x, y + 0.05, f'{base_label}\\n({count})', ha='center', va='bottom', \n",
    "               fontsize=10, fontweight='bold', zorder=3)\n",
    "        \n",
    "        drawn_positions[base_label] = (x, y)\n",
    "    \n",
    "    # Draw combination labels (child nodes)\n",
    "    child_y_offset = 0.3\n",
    "    for base_label in base_labels_sorted:\n",
    "        if base_label not in label_hierarchy:\n",
    "            continue\n",
    "            \n",
    "        combinations = label_hierarchy[base_label]\n",
    "        if not combinations:\n",
    "            continue\n",
    "            \n",
    "        base_x, base_y = base_positions[base_label]\n",
    "        \n",
    "        # Position child nodes\n",
    "        n_children = len(combinations)\n",
    "        if n_children == 1:\n",
    "            child_positions = [(base_x, base_y - child_y_offset)]\n",
    "        else:\n",
    "            child_spacing = 0.2 / max(1, n_children - 1)\n",
    "            start_x = base_x - 0.1\n",
    "            child_positions = [(start_x + i * child_spacing, base_y - child_y_offset) \n",
    "                             for i in range(n_children)]\n",
    "        \n",
    "        for (combo_label, combo_count), (child_x, child_y) in zip(combinations, child_positions):\n",
    "            # Skip single labels (already drawn as base)\n",
    "            if len(combo_label.split()) <= 1:\n",
    "                continue\n",
    "                \n",
    "            # Node size proportional to frequency\n",
    "            node_size = 15 + (combo_count / max_count) * 60\n",
    "            \n",
    "            # Color mixing based on components\n",
    "            components = combo_label.split()\n",
    "            if len(components) >= 2:\n",
    "                # Blend colors of components\n",
    "                color1 = colors.get(components[0], '#1f77b4')\n",
    "                color2 = colors.get(components[1], '#1f77b4')\n",
    "                # Simple color mixing (just use first component's color with alpha)\n",
    "                color = color1\n",
    "            else:\n",
    "                color = colors.get(base_label, '#1f77b4')\n",
    "            \n",
    "            # Draw child node\n",
    "            circle = plt.Circle((child_x, child_y), node_size/1000, color=color, alpha=0.5, zorder=2)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            # Draw connection line\n",
    "            ax.plot([base_x, child_x], [base_y - 0.02, child_y + 0.02], \n",
    "                   'k-', alpha=0.3, linewidth=1, zorder=1)\n",
    "            \n",
    "            # Label\n",
    "            ax.text(child_x, child_y - 0.08, f'{combo_label}\\n({combo_count})', \n",
    "                   ha='center', va='top', fontsize=8, zorder=3)\n",
    "    \n",
    "    # Additional combinations not tied to single base\n",
    "    remaining_combos = []\n",
    "    for label, count in label_counts.items():\n",
    "        if pd.isna(label):\n",
    "            continue\n",
    "        components = [c.strip() for c in str(label).split() if c.strip()]\n",
    "        if len(components) > 1:\n",
    "            # Check if it's already handled\n",
    "            primary = components[0]\n",
    "            if (label, count) not in label_hierarchy.get(primary, []):\n",
    "                remaining_combos.append((label, count))\n",
    "    \n",
    "    # Draw remaining complex combinations at the bottom\n",
    "    if remaining_combos:\n",
    "        bottom_y = 0.1\n",
    "        n_remaining = len(remaining_combos)\n",
    "        remaining_spacing = 0.8 / max(1, n_remaining - 1) if n_remaining > 1 else 0\n",
    "        \n",
    "        for i, (combo_label, combo_count) in enumerate(remaining_combos):\n",
    "            x_pos = 0.1 + i * remaining_spacing\n",
    "            node_size = 15 + (combo_count / max_count) * 60\n",
    "            \n",
    "            # Use neutral color for complex combinations\n",
    "            circle = plt.Circle((x_pos, bottom_y), node_size/1000, color='gray', alpha=0.6, zorder=2)\n",
    "            ax.add_patch(circle)\n",
    "            \n",
    "            ax.text(x_pos, bottom_y - 0.05, f'{combo_label}\\n({combo_count})', \n",
    "                   ha='center', va='top', fontsize=7, zorder=3)\n",
    "    \n",
    "    # Formatting\n",
    "    ax.set_xlim(0, 1)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_aspect('equal')\n",
    "    ax.axis('off')\n",
    "    ax.set_title('Hierarchical Label Structure\\n(Node size = frequency, Lines = relationships)', \n",
    "                fontsize=14, fontweight='bold', pad=20)\n",
    "    \n",
    "    # Legend\n",
    "    legend_elements = []\n",
    "    for label, color in colors.items():\n",
    "        if label in base_labels:\n",
    "            legend_elements.append(plt.Line2D([0], [0], marker='o', color='w', \n",
    "                                            markerfacecolor=color, markersize=10, label=label))\n",
    "    \n",
    "    if legend_elements:\n",
    "        ax.legend(handles=legend_elements, loc='upper right', bbox_to_anchor=(0.98, 0.98))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    tree_out = out_dir / 'label_hierarchy_tree.png'\n",
    "    plt.savefig(tree_out, dpi=150, bbox_inches='tight')\n",
    "    plt.close()\n",
    "    print(f'Saved label tree visualization -> {tree_out}')\n",
    "    \n",
    "    # Create summary statistics\n",
    "    stats = {\n",
    "        'total_images': len(merged),\n",
    "        'unique_combinations': len(label_counts),\n",
    "        'base_labels': sorted(base_labels),\n",
    "        'most_common': label_counts.head(10).to_dict()\n",
    "    }\n",
    "    \n",
    "    stats_out = out_dir / 'label_hierarchy_stats.json'\n",
    "    with open(stats_out, 'w') as f:\n",
    "        json.dump(stats, f, indent=2)\n",
    "    print(f'Saved label statistics -> {stats_out}')\n",
    "    \n",
    "    return label_counts, base_labels\n",
    "\n",
    "# Create the visualization\n",
    "if 'merged' in globals():\n",
    "    label_counts, base_labels = create_label_tree_visualization()\n",
    "    print(f\"\\nLabel hierarchy summary:\")\n",
    "    print(f\"Base labels: {', '.join(sorted(base_labels))}\")\n",
    "    print(f\"Top 5 combinations: {list(label_counts.head().index)}\")\n",
    "else:\n",
    "    print(\"Run label loading first to create tree visualization.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
